{
  "schema_version": "2.0",
  "meta": {
    "generated_at": "2026-01-10T12:04:36.377Z",
    "slug": "chakshugautam-advanced-evaluation",
    "source_url": "https://github.com/ChakshuGautam/games/tree/main/.claude/skills/advanced-evaluation",
    "source_ref": "main",
    "model": "claude",
    "analysis_version": "2.0.0",
    "source_type": "community",
    "content_hash": "e5f93428bd9f11f0133be742a6e09cd2d53a3f1fa2148fcfed1f6d80699e0b1b",
    "tree_hash": "11298b797596b62f832d1e84fce452e303c189f3b4036aa6ef3bd36b7360c76b"
  },
  "skill": {
    "name": "advanced-evaluation",
    "description": "This skill should be used when the user asks to \"implement LLM-as-judge\", \"compare model outputs\", \"create evaluation rubrics\", \"mitigate evaluation bias\", or mentions direct scoring, pairwise comparison, position bias, evaluation pipelines, or automated quality assessment.",
    "summary": "This skill should be used when the user asks to \"implement LLM-as-judge\", \"compare model outputs\", \"...",
    "icon": "ðŸ“Š",
    "version": "1.0.0",
    "author": "ChakshuGautam",
    "license": "MIT",
    "category": "data",
    "tags": [
      "llm-evaluation",
      "quality-assessment",
      "bias-mitigation",
      "llm-as-judge",
      "rubrics"
    ],
    "supported_tools": [
      "claude",
      "codex",
      "claude-code"
    ],
    "risk_factors": []
  },
  "security_audit": {
    "risk_level": "safe",
    "is_blocked": false,
    "safe_to_publish": true,
    "summary": "Pure prompt-based skill with no executable code. All content is documentation and educational pseudocode demonstrating evaluation patterns. No network calls, file system access, or command execution detected.",
    "risk_factor_evidence": [],
    "critical_findings": [],
    "high_findings": [],
    "medium_findings": [],
    "low_findings": [],
    "dangerous_patterns": [],
    "files_scanned": 5,
    "total_lines": 1730,
    "audit_model": "claude",
    "audited_at": "2026-01-10T12:04:36.377Z"
  },
  "content": {
    "user_title": "Evaluate LLM outputs with LLM judges",
    "value_statement": "Building reliable LLM evaluation systems is challenging due to systematic biases. This skill provides production-ready patterns for direct scoring, pairwise comparison, and bias mitigation to create trustworthy automated assessment pipelines.",
    "seo_keywords": [
      "llm evaluation",
      "llm-as-judge",
      "claude evaluation",
      "codex quality assessment",
      "pairwise comparison",
      "bias mitigation",
      "evaluation rubrics",
      "direct scoring",
      "automated assessment"
    ],
    "actual_capabilities": [
      "Design direct scoring systems with criteria and calibrated scales",
      "Implement pairwise comparison with position bias mitigation",
      "Create domain-specific evaluation rubrics with clear levels",
      "Select appropriate metrics for different evaluation tasks",
      "Detect and mitigate common LLM judge biases",
      "Build multi-layer evaluation pipelines with confidence scoring"
    ],
    "limitations": [
      "Does not execute LLM calls - provides patterns only",
      "Requires separate LLM API integration for implementation",
      "Does not provide pre-built evaluation datasets or gold standards",
      "Bias mitigation reduces but does not eliminate all biases"
    ],
    "use_cases": [
      {
        "target_user": "ML Engineers",
        "title": "Build evaluation pipelines",
        "description": "Create automated quality assessment systems for LLM outputs with bias mitigation"
      },
      {
        "target_user": "Product Managers",
        "title": "Compare model versions",
        "description": "Design A/B tests to evaluate which model produces better responses"
      },
      {
        "target_user": "AI Researchers",
        "title": "Validate evaluation methods",
        "description": "Test and calibrate LLM-based judges against human judgments"
      }
    ],
    "prompt_templates": [
      {
        "title": "Direct scoring setup",
        "scenario": "Rate response quality",
        "prompt": "Create a direct scoring evaluation system for [task type]. Include: 1) 3-5 criteria with descriptions, 2) 1-5 scale with level descriptions, 3) chain-of-thought scoring prompt that requires evidence before each score, 4) output format for structured results."
      },
      {
        "title": "Pairwise comparison",
        "scenario": "Compare two responses",
        "prompt": "Design a pairwise comparison evaluation between Response A and Response B for [task]. Include: 1) position swapping protocol, 2) anti-bias prompt instructions, 3) confidence calibration based on consistency, 4) JSON output format with per-criterion analysis."
      },
      {
        "title": "Rubric generation",
        "scenario": "Create scoring guide",
        "prompt": "Generate a domain-specific rubric for [criterion] in [domain]. Include: 1) score levels 1-5 with clear boundaries, 2) observable characteristics for each level, 3) edge case guidance, 4) scoring guidelines for consistency."
      },
      {
        "title": "Bias audit",
        "scenario": "Detect evaluation bias",
        "prompt": "Create a bias detection and monitoring system for LLM judges. Include: 1) position consistency check, 2) length-score correlation analysis, 3) confidence calibration, 4) reporting format for bias indicators."
      }
    ],
    "output_examples": [
      {
        "input": "Evaluate this customer service response using direct scoring with accuracy, clarity, and completeness criteria",
        "output": [
          "Accuracy: 4/5 - Correct solution provided with appropriate steps",
          "Clarity: 5/5 - Clear structure, good formatting, easy to follow",
          "Completeness: 4/5 - Addressed main issue, minor follow-up suggestion missing",
          "Weighted Score: 4.1/5",
          "Confidence: High - Strong evidence for each score"
        ]
      }
    ],
    "best_practices": [
      "Always require justification before scores - chain-of-thought improves reliability by 15-25%",
      "Swap positions in pairwise comparison to mitigate first-position bias",
      "Calibrate confidence to position consistency and evidence strength"
    ],
    "anti_patterns": [
      "Scoring without justification - difficult to debug or improve evaluations",
      "Single-pass pairwise comparison - position bias corrupts results",
      "Ignoring confidence calibration - high-confidence wrong judgments are dangerous"
    ],
    "faq": [
      {
        "question": "Which is better: direct scoring or pairwise comparison?",
        "answer": "Use direct scoring for objective criteria with ground truth. Use pairwise comparison for subjective preferences. Research shows pairwise achieves higher human agreement for preferences."
      },
      {
        "question": "How many evaluation criteria should I use?",
        "answer": "Use 3-5 criteria per evaluation. More criteria reduce reliability. Each criterion should measure one distinct aspect with clear definitions."
      },
      {
        "question": "Can I use the same model for generation and evaluation?",
        "answer": "Not recommended - models show self-enhancement bias. Use a different model family for evaluation, or acknowledge this limitation in results."
      },
      {
        "question": "How do I handle ties in pairwise comparison?",
        "answer": "When position-swapped passes disagree, return TIE with 0.5 confidence. Track tie rates - high rates may indicate poorly differentiated responses or criteria."
      },
      {
        "question": "How do I validate my evaluation system?",
        "answer": "Compare automated judgments against human evaluations using Spearman correlation or Cohen kappa. Target 0.7+ correlation for production use."
      },
      {
        "question": "What confidence level is acceptable?",
        "answer": "For production systems, require 0.8+ confidence for high-stakes decisions. Use human review for low-confidence cases. Calibrate confidence to evidence strength and position consistency."
      }
    ]
  },
  "file_structure": [
    {
      "name": "references",
      "type": "dir",
      "path": "references",
      "children": [
        {
          "name": "bias-mitigation.md",
          "type": "file",
          "path": "references/bias-mitigation.md"
        },
        {
          "name": "implementation-patterns.md",
          "type": "file",
          "path": "references/implementation-patterns.md"
        },
        {
          "name": "metrics-guide.md",
          "type": "file",
          "path": "references/metrics-guide.md"
        }
      ]
    },
    {
      "name": "scripts",
      "type": "dir",
      "path": "scripts",
      "children": [
        {
          "name": "evaluation_example.py",
          "type": "file",
          "path": "scripts/evaluation_example.py"
        }
      ]
    },
    {
      "name": "SKILL.md",
      "type": "file",
      "path": "SKILL.md"
    }
  ]
}
