{
  "schema_version": "2.0",
  "meta": {
    "generated_at": "2026-01-10T12:16:24.011Z",
    "slug": "chakshugautam-evaluation",
    "source_url": "https://github.com/ChakshuGautam/games/tree/main/.claude/skills/evaluation",
    "source_ref": "main",
    "model": "claude",
    "analysis_version": "2.0.0",
    "source_type": "community",
    "content_hash": "25d3bac5a761f7cd7fd177b00b11638b6415e0138d13c5493ce4090cdeb1c9fc",
    "tree_hash": "ddb75fdaca4bebbdf33ee142f6be4c6b5c856c33d918080c93772aaf81fd7161"
  },
  "skill": {
    "name": "evaluation",
    "description": "This skill should be used when the user asks to \"evaluate agent performance\", \"build test framework\", \"measure agent quality\", \"create evaluation rubrics\", or mentions LLM-as-judge, multi-dimensional evaluation, agent testing, or quality gates for agent pipelines.",
    "summary": "This skill should be used when the user asks to \"evaluate agent performance\", \"build test framework\"...",
    "icon": "ðŸ“Š",
    "version": "1.0.0",
    "author": "ChakshuGautam",
    "license": "MIT",
    "category": "coding",
    "tags": [
      "evaluation",
      "testing",
      "metrics",
      "quality-assurance",
      "agent-performance"
    ],
    "supported_tools": [
      "claude",
      "codex",
      "claude-code"
    ],
    "risk_factors": []
  },
  "security_audit": {
    "risk_level": "safe",
    "is_blocked": false,
    "safe_to_publish": true,
    "summary": "The evaluation skill contains no security vulnerabilities. It's a clean Python implementation that operates entirely in memory without network access, file system operations, or code execution capabilities. The code matches its stated purpose of providing evaluation frameworks for AI agents.",
    "risk_factor_evidence": [],
    "critical_findings": [],
    "high_findings": [],
    "medium_findings": [],
    "low_findings": [],
    "dangerous_patterns": [],
    "files_scanned": 3,
    "total_lines": 820,
    "audit_model": "claude",
    "audited_at": "2026-01-10T12:16:24.011Z"
  },
  "content": {
    "user_title": "Evaluate AI Agent Performance",
    "value_statement": "Building reliable AI agents requires systematic evaluation. This skill provides multi-dimensional evaluation frameworks to measure agent quality, catch regressions, and validate improvements.",
    "seo_keywords": [
      "AI agent evaluation",
      "Claude Code testing",
      "LLM-as-judge",
      "agent performance metrics",
      "multi-dimensional rubrics",
      "quality gates",
      "regression testing",
      "Claude evaluation",
      "Codex testing"
    ],
    "actual_capabilities": [
      "Creates multi-dimensional evaluation rubrics with weighted scoring",
      "Implements LLM-as-judge evaluation for scalable testing",
      "Provides test set management with complexity stratification",
      "Monitors production performance with automated sampling",
      "Calculates weighted scores across factual accuracy, completeness, citation accuracy, source quality, and tool efficiency",
      "Generates evaluation reports with pass/fail determinations"
    ],
    "limitations": [
      "Requires manual setup of ground truth data for accurate evaluation",
      "Heuristic-based scoring in demonstration mode, not production-ready",
      "No persistence layer - evaluation data stored in memory only",
      "Limited to text-based evaluation, no multi-modal support"
    ],
    "use_cases": [
      {
        "target_user": "AI engineers",
        "title": "Agent Quality Assurance",
        "description": "Systematically test agent performance across multiple dimensions before deployment"
      },
      {
        "target_user": "Product managers",
        "title": "Performance Monitoring",
        "description": "Track agent quality metrics over time and catch regressions early"
      },
      {
        "target_user": "Research teams",
        "title": "Context Engineering Validation",
        "description": "Measure the impact of context engineering choices on agent performance"
      }
    ],
    "prompt_templates": [
      {
        "title": "Basic Agent Evaluation",
        "scenario": "Simple quality check",
        "prompt": "Evaluate this agent response for factual accuracy and completeness: [AGENT_OUTPUT]"
      },
      {
        "title": "Multi-Dimensional Assessment",
        "scenario": "Comprehensive evaluation",
        "prompt": "Create an evaluation rubric for my research agent covering accuracy, source quality, and tool efficiency"
      },
      {
        "title": "Production Monitoring Setup",
        "scenario": "Continuous evaluation",
        "prompt": "Set up automated evaluation sampling for my production agent with 1% sample rate"
      },
      {
        "title": "Test Framework Design",
        "scenario": "Systematic testing",
        "prompt": "Design a test set for my data analysis agent with simple, medium, and complex test cases"
      }
    ],
    "output_examples": [
      {
        "input": "Evaluate my agent's response about climate change",
        "output": [
          "Overall Score: 0.82 (Good)",
          "Factual Accuracy: 0.9 - All climate claims verified against scientific consensus",
          "Completeness: 0.8 - Covered main aspects but missed regional variations",
          "Citation Accuracy: 0.7 - Missing some source attributions",
          "Source Quality: 0.85 - Used authoritative IPCC and NASA sources",
          "Tool Efficiency: 0.9 - Optimal search strategy with 3 targeted queries",
          "Result: PASSED (threshold: 0.7)"
        ]
      }
    ],
    "best_practices": [
      "Use multi-dimensional rubrics instead of single metrics to capture agent quality comprehensively",
      "Test with realistic token budgets and context sizes that match production usage",
      "Supplement automated LLM evaluation with periodic human review for edge cases"
    ],
    "anti_patterns": [
      "Overfitting evaluations to specific execution paths rather than outcome-focused assessment",
      "Ignoring context-dependent failures that emerge only after extended interactions",
      "Relying solely on single-metric evaluation that misses important quality dimensions"
    ],
    "faq": [
      {
        "question": "Is this compatible with my existing agent framework?",
        "answer": "Yes, the evaluation framework is framework-agnostic and works with any agent that produces text output"
      },
      {
        "question": "How many test cases do I need for reliable evaluation?",
        "answer": "Start with 20-30 diverse cases covering different complexity levels, expand based on your use case"
      },
      {
        "question": "Can I integrate this with my CI/CD pipeline?",
        "answer": "Yes, the EvaluationRunner class is designed for automated pipelines and returns structured results"
      },
      {
        "question": "Is my data safe during evaluation?",
        "answer": "All evaluation happens locally in memory with no external data transmission"
      },
      {
        "question": "What if my agent fails evaluation?",
        "answer": "The framework provides detailed dimension scores to identify specific improvement areas"
      },
      {
        "question": "How does this compare to manual evaluation?",
        "answer": "LLM-as-judge scales better while maintaining 80-90% agreement with human evaluators"
      }
    ]
  },
  "file_structure": [
    {
      "name": "references",
      "type": "dir",
      "path": "references",
      "children": [
        {
          "name": "metrics.md",
          "type": "file",
          "path": "references/metrics.md"
        }
      ]
    },
    {
      "name": "scripts",
      "type": "dir",
      "path": "scripts",
      "children": [
        {
          "name": "evaluator.py",
          "type": "file",
          "path": "scripts/evaluator.py"
        }
      ]
    },
    {
      "name": "SKILL.md",
      "type": "file",
      "path": "SKILL.md"
    }
  ]
}
