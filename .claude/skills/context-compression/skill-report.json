{
  "schema_version": "2.0",
  "meta": {
    "generated_at": "2026-01-10T12:05:47.120Z",
    "slug": "chakshugautam-context-compression",
    "source_url": "https://github.com/ChakshuGautam/games/tree/main/.claude/skills/context-compression",
    "source_ref": "main",
    "model": "claude",
    "analysis_version": "2.0.0",
    "source_type": "community",
    "content_hash": "bc398950888ae73df9651f44f2fb51acea9409362d736592c771e38d236022ae",
    "tree_hash": "23d17e126580902ee92b9baf93e7d03d7e0aa81f7f53e2a88ec65ad60c7a7774"
  },
  "skill": {
    "name": "context-compression",
    "description": "This skill should be used when the user asks to \"compress context\", \"summarize conversation history\", \"implement compaction\", \"reduce token usage\", or mentions context compression, structured summarization, tokens-per-task optimization, or long-running agent sessions exceeding context limits.",
    "summary": "This skill should be used when the user asks to \"compress context\", \"summarize conversation history\"...",
    "icon": "üóúÔ∏è",
    "version": "1.0.0",
    "author": "ChakshuGautam",
    "license": "MIT",
    "category": "coding",
    "tags": [
      "context-compression",
      "token-optimization",
      "ai-agents",
      "conversation-summarization",
      "evaluation"
    ],
    "supported_tools": [
      "claude",
      "codex",
      "claude-code"
    ],
    "risk_factors": []
  },
  "security_audit": {
    "risk_level": "safe",
    "is_blocked": false,
    "safe_to_publish": true,
    "summary": "This skill contains only documentation and evaluation utilities. No network calls, file system access beyond its directory, or code execution capabilities. The Python script is a demonstration stub with no actual LLM API calls.",
    "risk_factor_evidence": [],
    "critical_findings": [],
    "high_findings": [],
    "medium_findings": [],
    "low_findings": [],
    "dangerous_patterns": [],
    "files_scanned": 3,
    "total_lines": 714,
    "audit_model": "claude",
    "audited_at": "2026-01-10T12:05:47.119Z"
  },
  "content": {
    "user_title": "Compress AI Agent Context Efficiently",
    "value_statement": "Long AI agent sessions quickly exceed context windows, causing failures. This skill provides proven compression strategies that preserve critical information while reducing tokens by 98%+.",
    "seo_keywords": [
      "context compression",
      "AI agent optimization",
      "token reduction",
      "Claude context window",
      "conversation summarization",
      "Codex compression",
      "Claude Code optimization",
      "agent memory management",
      "structured summarization",
      "token efficiency"
    ],
    "actual_capabilities": [
      "Implements anchored iterative summarization with 98.6% compression ratio",
      "Provides probe-based evaluation framework for compression quality",
      "Generates structured summaries with explicit file tracking sections",
      "Evaluates compression across 6 dimensions: accuracy, context, artifacts, completeness, continuity, instructions",
      "Includes Python implementation for compression evaluation"
    ],
    "limitations": [
      "Python evaluator uses stub implementation - needs actual LLM API integration",
      "Artifact trail tracking is universally weak across all compression methods",
      "Requires manual integration into agent workflows",
      "Evaluation framework needs ground truth data for optimal results"
    ],
    "use_cases": [
      {
        "target_user": "AI agent developers",
        "title": "Build Context-Aware Agents",
        "description": "Implement compression in agents that handle long coding sessions or document analysis tasks exceeding context limits."
      },
      {
        "target_user": "ML engineers",
        "title": "Evaluate Compression Quality",
        "description": "Use probe-based evaluation to measure how well different compression methods preserve task-critical information."
      },
      {
        "target_user": "DevOps teams",
        "title": "Optimize Production Agents",
        "description": "Deploy structured summarization to reduce token costs while maintaining agent performance on complex tasks."
      }
    ],
    "prompt_templates": [
      {
        "title": "Basic Compression",
        "scenario": "Agent session hitting context limits",
        "prompt": "Compress this conversation history using structured summarization. Include sections for: session intent, files modified, decisions made, current state, and next steps."
      },
      {
        "title": "Evaluation Setup",
        "scenario": "Testing compression quality",
        "prompt": "Generate probe questions for this compressed context to test if key information was preserved: file paths, error messages, decisions, and next steps."
      },
      {
        "title": "Iterative Update",
        "scenario": "Updating existing summary",
        "prompt": "Merge this new conversation span into the existing structured summary. Update only the relevant sections while preserving information from previous compression cycles."
      },
      {
        "title": "Production Integration",
        "scenario": "Implementing in agent workflow",
        "prompt": "Design a compression trigger strategy for a coding agent that activates at 75% context utilization, using anchored iterative summarization with file tracking."
      }
    ],
    "output_examples": [
      {
        "input": "Compress a debugging session where we fixed a 401 error by updating Redis configuration",
        "output": [
          "## Session Intent",
          "Debug 401 Unauthorized error on /api/auth/login endpoint",
          "",
          "## Files Modified",
          "- config/redis.ts: Fixed connection pooling configuration",
          "- services/session.service.ts: Added retry logic for transient failures",
          "",
          "## Decisions Made",
          "- Using Redis connection pool instead of per-request connections",
          "- Retry logic with exponential backoff for transient failures",
          "",
          "## Current State",
          "14 tests passing, 2 failing (mock setup issues)",
          "",
          "## Next Steps",
          "1. Fix remaining test failures",
          "2. Run full test suite",
          "3. Deploy to staging"
        ]
      }
    ],
    "best_practices": [
      "Trigger compression at 70-80% context utilization to balance quality and token savings",
      "Use structured summaries with explicit sections to force preservation of critical information",
      "Monitor tokens-per-task metric rather than tokens-per-request to measure true efficiency",
      "Implement probe-based evaluation to verify compression quality before deployment"
    ],
    "anti_patterns": [
      "Don't use aggressive compression that loses file paths and error messages - causes expensive re-fetching",
      "Avoid regenerating entire summaries on each compression - use incremental merging instead",
      "Don't rely solely on lexical metrics like ROUGE - use functional evaluation with probes"
    ],
    "faq": [
      {
        "question": "Which compression method should I use?",
        "answer": "Use anchored iterative summarization for coding tasks - it provides best quality retention with 98.6% compression ratio."
      },
      {
        "question": "How do I know if compression is working?",
        "answer": "Use probe-based evaluation: ask specific questions about files, errors, and decisions to verify critical information was preserved."
      },
      {
        "question": "Can I use this with any AI model?",
        "answer": "Yes, the strategies are model-agnostic. The evaluation framework works with any LLM that can act as a judge."
      },
      {
        "question": "Why is artifact tracking so weak?",
        "answer": "File tracking is inherently difficult for general summarization. Consider implementing separate artifact indexing for critical file operations."
      },
      {
        "question": "How much token savings can I expect?",
        "answer": "98-99% compression is typical. Structured methods retain 0.7% more tokens but provide 0.35 points better quality scores."
      },
      {
        "question": "When should I compress vs. start a new session?",
        "answer": "Compress when you need continuity. Start fresh when switching to unrelated tasks or when compression quality drops below 3.5/5.0."
      }
    ]
  },
  "file_structure": [
    {
      "name": "references",
      "type": "dir",
      "path": "references",
      "children": [
        {
          "name": "evaluation-framework.md",
          "type": "file",
          "path": "references/evaluation-framework.md"
        }
      ]
    },
    {
      "name": "scripts",
      "type": "dir",
      "path": "scripts",
      "children": [
        {
          "name": "compression_evaluator.py",
          "type": "file",
          "path": "scripts/compression_evaluator.py"
        }
      ]
    },
    {
      "name": "SKILL.md",
      "type": "file",
      "path": "SKILL.md"
    }
  ]
}
